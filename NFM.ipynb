{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/MarsImmortal/neural_factorization_machine.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSDvOmT0c2S6",
        "outputId": "eb861b60-aa8e-42bd-b4cb-02abf5164c1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neural_factorization_machine'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 127 (delta 54), reused 83 (delta 34), pack-reused 24 (from 1)\u001b[K\n",
            "Receiving objects: 100% (127/127), 20.08 MiB | 13.49 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd neural_factorization_machine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwTt1k9CUpDJ",
        "outputId": "396f26cd-86f2-4bf9-8824-7a882b287a64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neural_factorization_machine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class LoadData(object):\n",
        "    '''given the path of data, return the data format for DeepFM\n",
        "    :param path\n",
        "    return:\n",
        "    Train_data: a dictionary, 'Y' refers to a list of y values; 'X' refers to a list of features_M dimension vectors with 0 or 1 entries\n",
        "    Test_data: same as Train_data\n",
        "    Validation_data: same as Train_data\n",
        "    '''\n",
        "\n",
        "    def __init__(self, path, dataset, loss_type):\n",
        "        self.path = path + dataset + \"/\"\n",
        "        self.trainfile = self.path + dataset +\".train.libfm\"\n",
        "        self.testfile = self.path + dataset + \".test.libfm\"\n",
        "        self.validationfile = self.path + dataset + \".validation.libfm\"\n",
        "        self.features_M = self.map_features()\n",
        "        self.Train_data, self.Validation_data, self.Test_data = self.construct_data(loss_type)\n",
        "\n",
        "    def map_features(self):\n",
        "        self.features = {}\n",
        "        self.read_features(self.trainfile)\n",
        "        self.read_features(self.testfile)\n",
        "        self.read_features(self.validationfile)\n",
        "        return len(self.features)\n",
        "\n",
        "    def read_features(self, file):\n",
        "        with open(file) as f:\n",
        "            line = f.readline()\n",
        "            i = len(self.features)\n",
        "            while line:\n",
        "                items = line.strip().split(' ')\n",
        "                for item in items[1:]:\n",
        "                    if item not in self.features:\n",
        "                        self.features[item] = i\n",
        "                        i += 1\n",
        "                line = f.readline()\n",
        "\n",
        "    def construct_data(self, loss_type):\n",
        "        X_, Y_, Y_for_logloss = self.read_data(self.trainfile)\n",
        "        Train_data = self.construct_dataset(X_, Y_for_logloss if loss_type == 'log_loss' else Y_)\n",
        "        X_, Y_, Y_for_logloss = self.read_data(self.validationfile)\n",
        "        Validation_data = self.construct_dataset(X_, Y_for_logloss if loss_type == 'log_loss' else Y_)\n",
        "        X_, Y_, Y_for_logloss = self.read_data(self.testfile)\n",
        "        Test_data = self.construct_dataset(X_, Y_for_logloss if loss_type == 'log_loss' else Y_)\n",
        "        return Train_data, Validation_data, Test_data\n",
        "\n",
        "    def read_data(self, file):\n",
        "        X_ = []\n",
        "        Y_ = []\n",
        "        Y_for_logloss = []\n",
        "        with open(file) as f:\n",
        "            line = f.readline()\n",
        "            while line:\n",
        "                items = line.strip().split(' ')\n",
        "                Y_.append(float(items[0]))\n",
        "                v = 1.0 if float(items[0]) > 0 else 0.0\n",
        "                Y_for_logloss.append(v)\n",
        "                X_.append([self.features[item] for item in items[1:]])\n",
        "                line = f.readline()\n",
        "        return X_, Y_, Y_for_logloss\n",
        "\n",
        "    def construct_dataset(self, X_, Y_):\n",
        "        Data_Dic = {}\n",
        "        X_lens = [len(line) for line in X_]\n",
        "        indexs = np.argsort(X_lens)\n",
        "        Data_Dic['Y'] = [Y_[i] for i in indexs]\n",
        "        Data_Dic['X'] = [X_[i] for i in indexs]\n",
        "        return Data_Dic\n",
        "\n",
        "    def truncate_features(self):\n",
        "        num_variable = len(self.Train_data['X'][0])\n",
        "        for i in range(len(self.Train_data['X'])):\n",
        "            num_variable = min(num_variable, len(self.Train_data['X'][i]))\n",
        "        for i in range(len(self.Train_data['X'])):\n",
        "            self.Train_data['X'][i] = self.Train_data['X'][i][:num_variable]\n",
        "        for i in range(len(self.Validation_data['X'])):\n",
        "            self.Validation_data['X'][i] = self.Validation_data['X'][i][:num_variable]\n",
        "        for i in range(len(self.Test_data['X'])):\n",
        "            self.Test_data['X'][i] = self.Test_data['X'][i][:num_variable]\n",
        "        return num_variable\n"
      ],
      "metadata": {
        "id": "Uy3H_nctVX_k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NeuralFM(tf.keras.Model):\n",
        "    def __init__(self, feature_dim, hidden_factor, layers, keep_prob, loss_type='square_loss', activation='relu'):\n",
        "        super(NeuralFM, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(feature_dim, hidden_factor, mask_zero=False)\n",
        "        self.hidden_layers = []\n",
        "        for layer_size in layers:\n",
        "            self.hidden_layers.append(tf.keras.layers.Dense(layer_size, activation=activation))\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid' if loss_type == 'log_loss' else 'linear')\n",
        "        self.dropout_layers = [tf.keras.layers.Dropout(p) for p in keep_prob]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        fm_part = tf.reduce_sum(x, axis=1) ** 2 - tf.reduce_sum(x ** 2, axis=1)\n",
        "        fm_part = 0.5 * tf.reduce_sum(fm_part, axis=1, keepdims=True)\n",
        "\n",
        "        x = tf.reduce_sum(x, axis=1)\n",
        "        for layer, dropout in zip(self.hidden_layers, self.dropout_layers):\n",
        "            x = layer(x)\n",
        "            if training:\n",
        "                x = dropout(x)\n",
        "\n",
        "        x = tf.concat([x, fm_part], axis=1)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "d77UO0xkVbEL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "path = 'data/'\n",
        "dataset = 'frappe'\n",
        "loss_type = 'square_loss'  # or 'log_loss'\n",
        "\n",
        "loader = LoadData(path, dataset, loss_type)\n",
        "feature_dim = loader.features_M\n",
        "hidden_factor = 64\n",
        "layers = [64]\n",
        "keep_prob = [0.8, 0.5]\n",
        "\n",
        "# Create model\n",
        "model = NeuralFM(feature_dim, hidden_factor, layers, keep_prob, loss_type=loss_type, activation='relu')\n",
        "\n",
        "# Compile model\n",
        "optimizer = Adagrad(learning_rate=0.05)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy' if loss_type == 'log_loss' else 'mean_squared_error')\n",
        "\n",
        "# Prepare data\n",
        "def prepare_data(data):\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(data['X'], padding='post')\n",
        "    y = np.array(data['Y'])\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = prepare_data(loader.Train_data)\n",
        "X_val, y_val = prepare_data(loader.Validation_data)\n",
        "X_test, y_test = prepare_data(loader.Test_data)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "if loss_type == 'log_loss':\n",
        "    score = log_loss(y_test, y_pred)\n",
        "else:\n",
        "    score = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Test {loss_type} score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmGh_hj1Vbp5",
        "outputId": "c9e29644-5cbe-4912-ebbd-1cc5e2db3895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'neural_fm', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - loss: 0.5877 - val_loss: 0.3286\n",
            "Epoch 2/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.3049 - val_loss: 0.2847\n",
            "Epoch 3/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.2500 - val_loss: 0.2442\n",
            "Epoch 4/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.2037 - val_loss: 0.2207\n",
            "Epoch 5/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.1677 - val_loss: 0.2038\n",
            "Epoch 6/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - loss: 0.1406 - val_loss: 0.1940\n",
            "Epoch 7/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.1183 - val_loss: 0.1859\n",
            "Epoch 8/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.1005 - val_loss: 0.1808\n",
            "Epoch 9/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0867 - val_loss: 0.1761\n",
            "Epoch 10/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0742 - val_loss: 0.1737\n",
            "Epoch 11/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0646 - val_loss: 0.1713\n",
            "Epoch 12/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0564 - val_loss: 0.1714\n",
            "Epoch 13/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0500 - val_loss: 0.1701\n",
            "Epoch 14/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0432 - val_loss: 0.1709\n",
            "Epoch 15/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0381 - val_loss: 0.1718\n",
            "Epoch 16/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0341 - val_loss: 0.1728\n",
            "Epoch 17/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0309 - val_loss: 0.1733\n",
            "Epoch 18/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0274 - val_loss: 0.1747\n",
            "Epoch 19/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0248 - val_loss: 0.1764\n",
            "Epoch 20/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0224 - val_loss: 0.1775\n",
            "Epoch 21/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0201 - val_loss: 0.1793\n",
            "Epoch 22/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0185 - val_loss: 0.1799\n",
            "Epoch 23/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0169 - val_loss: 0.1817\n",
            "Epoch 24/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0155 - val_loss: 0.1828\n",
            "Epoch 25/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0142 - val_loss: 0.1844\n",
            "Epoch 26/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0129 - val_loss: 0.1851\n",
            "Epoch 27/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0120 - val_loss: 0.1868\n",
            "Epoch 28/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0110 - val_loss: 0.1875\n",
            "Epoch 29/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0105 - val_loss: 0.1890\n",
            "Epoch 30/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0096 - val_loss: 0.1899\n",
            "Epoch 31/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0092 - val_loss: 0.1910\n",
            "Epoch 32/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0084 - val_loss: 0.1918\n",
            "Epoch 33/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0080 - val_loss: 0.1930\n",
            "Epoch 34/200\n",
            "\u001b[1m3157/3157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0074 - val_loss: 0.1940\n",
            "Epoch 35/200\n",
            "\u001b[1m1106/3157\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0067"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u92v6joYSsxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}